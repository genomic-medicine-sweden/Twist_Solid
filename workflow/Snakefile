__author__ = "Jonas A"
__copyright__ = "Copyright 2021, Jonas A"
__email__ = "jonas.almlof@igp.uu.se"
__license__ = "GPL-3"


include: "rules/common.smk"
include: "rules/bcftools.smk"
include: "rules/call_small_cnv_deletions.smk"
include: "rules/call_small_cnv_amplifications.smk"
include: "rules/cnv_tsv_report.smk"
include: "rules/estimate_ctdna_fraction.smk"
include: "rules/exon_skipping.smk"
include: "rules/fix_vcf_ad_for_qci.smk"
include: "rules/hotspot_report.smk"
include: "rules/house_keeping_gene_coverage.smk"
include: "rules/purecn_modify_vcf.smk"
include: "rules/report_gene_fuse.smk"
include: "rules/report_fusions.smk"
include: "rules/sample_mixup_check.smk"


rule all:
    input:
        unpack(compile_output_list),


ruleorder: annotation_add_mosdepth_coverage_to_gvcf > snv_indels_bgzip
ruleorder: annotation_annotate_cnv > snv_indels_bgzip
ruleorder: alignment_bwa_mem_merge > alignment_samtools_sort
ruleorder: alignment_bwa_mem_merge_umi > alignment_samtools_sort_umi
ruleorder: alignment_fgbio_call_overlapping_consensus_bases_ffpe > alignment_samtools_sort
ruleorder: alignment_fgbio_call_overlapping_consensus_bases_ffpe > alignment_samtools_index
ruleorder: alignment_samtools_extract_reads_rna > alignment_samtools_extract_reads
ruleorder: alignment_samtools_sort_ffpe_fgbio > alignment_samtools_sort
ruleorder: alignment_samtools_sort_umi > alignment_samtools_sort
ruleorder: alignment_samtools_merge_bam > alignment_samtools_sort
ruleorder: filtering_bcftools_filter_include_region > snv_indels_bgzip
ruleorder: qc_samtools_stats_rna > qc_samtools_stats
ruleorder: qc_picard_collect_alignment_summary_metrics_rna > qc_picard_collect_alignment_summary_metrics
ruleorder: qc_picard_collect_hs_metrics_rna > qc_picard_collect_hs_metrics
ruleorder: snv_indels_bcftools_sort > snv_indels_bgzip
ruleorder: snv_indels_gatk_mutect2_gvcf > alignment_samtools_sort
ruleorder: snv_indels_bgzip > cnv_sv_bgzip
ruleorder: snv_indels_tabix > cnv_sv_tabix


wildcard_constraints:
    file="^(?!bam_dna|results|gvcf_dna).*",
    sample="[^/]+",
    type="T|N|R",


use rule bcftools_id_snps as bcftools_id_snps_dna with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        bed=config.get("bcftools_id_snps", {}).get("snps_bed", "missing bcftools_id_snps snps_bed file"),
        ref=config.get("reference", {}).get("fasta", "missing reference fasta"),
    wildcard_constraints:
        type="[TN]",


module prealignment:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/prealignment",
            path="workflow/Snakefile",
            tag=config["modules"]["prealignment"],
        )
    config:
        config


use rule * from prealignment exclude all, seqtk_subsample as prealignment_*


use rule fastp_pe from prealignment as prealignment_fastp_pe_arriba with:
    output:
        trimmed=temp(
            [
                "prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastq1.fastq.gz",
                "prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastq2.fastq.gz",
            ]
        ),
        html="prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastp.html",
        json="prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastp.json",
    params:
        adapters=lambda wildcards: " --adapter_sequence {} --adapter_sequence_r2 {} ".format(
            *get_fastq_adapter(units, wildcards).split(",")
        ),
        extra=config.get("fastp_pe_arriba", {}).get("extra", ""),
    log:
        "prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastq.fastq.gz.log",
    benchmark:
        repeat(
            "prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_fastq.fastq.gz.benchmark.tsv",
            config.get("fastp_pe_arriba", {}).get("benchmark_repeats", 1),
        )
    resources:
        mem_mb=config.get("fastp_pe_arriba", {}).get("mem_mb", config["default_resources"]["mem_mb"]),
        mem_per_cpu=config.get("fastp_pe_arriba", {}).get("mem_per_cpu", config["default_resources"]["mem_per_cpu"]),
        partition=config.get("fastp_pe_arriba", {}).get("partition", config["default_resources"]["partition"]),
        threads=config.get("fastp_pe_arriba", {}).get("threads", config["default_resources"]["threads"]),
        time=config.get("fastp_pe_arriba", {}).get("time", config["default_resources"]["time"]),
    threads: config.get("fastp_pe_arriba", {}).get("threads", config["default_resources"]["threads"])


use rule seqtk_subsample from prealignment as prealignment_seqtk_subsample_dna with:
    wildcard_constraints:
        type="T|N",


use rule seqtk_subsample from prealignment as prealignment_seqtk_subsample_rna with:
    params:
        extra=config.get("seqtk_subsample", {}).get("extra", "-2"),
        nr_reads_per_fastq=lambda wildcards: prealignment.get_nr_reads_per_fastq(
            config.get("seqtk_subsample", {}).get("nr_reads_rna", 1000000000), units, wildcards
        ),
        seed=config.get("seqtk_subsample", {}).get("seed", "-s100"),
    wildcard_constraints:
        type="R",


use rule seqtk_subsample from prealignment as prealignment_seqtk_subsample_arriba with:
    input:
        fastq="prealignment/fastp_pe_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_{read}.fastq.gz",
    output:
        fastq=temp("prealignment/seqtk_subsample_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_{read}.ds.fastq.gz"),
    params:
        extra=config.get("seqtk_subsample", {}).get("extra", "-2"),
        nr_reads_per_fastq=lambda wildcards: prealignment.get_nr_reads_per_fastq(
            config.get("seqtk_subsample", {}).get("nr_reads_rna", 1000000000), units, wildcards
        ),
        seed=config.get("seqtk_subsample", {}).get("seed", "-s100"),
    log:
        "prealignment/seqtk_subsample_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_{read}.ds.fastq.log",
    benchmark:
        repeat(
            "prealignment/seqtk_subsample_arriba/{sample}_{type}_{flowcell}_{lane}_{barcode}_{read}.ds.fastq.benchmark.tsv",
            config.get("seqtk_subsample", {}).get("benchmark_repeats", 1),
        )


use rule merged from prealignment as prealignment_merged_arriba with:
    input:
        fastq=merged_input_arriba,
    output:
        fastq=temp("prealignment/merged_arriba/{sample}_{type}_{read}.fastq.gz"),
    log:
        "prealignment/merged_arriba/{sample}_{type}_{read}.fastq.gz.log",
    benchmark:
        repeat(
            "prealignment/merged_arriba/{sample}_{type}_{read}.fastq.gz.benchmark.tsv",
            config.get("merged_arriba", {}).get("benchmark_repeats", 1),
        )


module alignment:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/alignment",
            path="workflow/Snakefile",
            tag=config["modules"]["alignment"],
        )
    config:
        config


use rule * from alignment exclude bwa_mem_merge, samtools_merge_bam, all as alignment_*


use rule samtools_index from alignment as alignment_samtools_index with:
    output:
        bai="{file}.bam.bai",
    wildcard_constraints:
        file="^(?:snv_indels/|alignment/|fusions/|bam_rna/)(?:(?!_unsorted).)+",


use rule samtools_sort from alignment as alignment_samtools_sort with:
    wildcard_constraints:
        file="^(?:snv_indels/|alignment/|fusions/)(?:(?!.*_unsorted).)+",


use rule samtools_sort_umi from alignment as alignment_samtools_sort_ffpe_fgbio with:
    input:
        bam="alignment/samtools_filter_reads/{sample}_{type}_unsorted.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}.bam"),


use rule samtools_merge_bam from alignment as alignment_samtools_merge_bam with:
    output:
        bam=temp("alignment/samtools_merge_bam/{sample}_{type}.bam"),
    params:
        extra=config.get("samtools_merge_bam", {}).get("extra", ""),
    log:
        "alignment/samtools_merge_bam/{sample}_{type}.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/samtools_merge_bam/{sample}_{type}.bam.benchmark.tsv",
            config.get("samtools_merge_bam", {}).get("benchmark_repeats", 1),
        )


use rule bwa_mem_merge from alignment as alignment_bwa_mem_merge with:
    output:
        bam=temp("alignment/bwa_mem/{sample}_{type}.bam"),
    params:
        config.get("bwa_mem_merge", {}).get("extra", ""),
    log:
        "alignment/bwa_mem/{sample}_{type}.bam.log",
    benchmark:
        repeat(
            "alignment/bwa_mem/{sample}_{type}.bam.benchmark.tsv",
            config.get("bwa_mem_merge", {}).get("benchmark_repeats", 1),
        )


use rule bwa_mem_merge from alignment as alignment_bwa_mem_merge_umi with:
    output:
        bam=temp("alignment/bwa_mem/{sample}_{type}.umi_unsorted.bam"),
    params:
        extra=config.get("bwa_mem_merge", {}).get("extra", ""),
    log:
        "alignment/bwa_mem/{sample}_{type}_unsorted.umi.bam.log",
    benchmark:
        repeat(
            "alignment/bwa_mem/{sample}_{type}_unsorted.umi.bam.benchmark.tsv",
            config.get("bwa_mem_merge", {}).get("benchmark_repeats", 1),
        )


use rule fgbio_call_overlapping_consensus_bases from alignment as alignment_fgbio_call_overlapping_consensus_bases_ffpe with:
    input:
        bam="alignment/samtools_extract_reads/{sample}_{type}_{chr}.bam",
        ref=config.get("reference", {}).get("fasta", ""),
    output:
        bam=temp("alignment/fgbio_call_overlapping_consensus_bases/{sample}_{type}_{chr}.bam"),
        metrics=temp("alignment/fgbio_call_overlapping_consensus_bases/{sample}_{type}_{chr}.metrics.txt"),
    log:
        "alignment/fgbio_call_overlapping_consensus_bases/{sample}_{type}_{chr}.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/fgbio_call_overlapping_consensus_bases/{sample}_{type}_{chr}.bam.benchmark.tsv",
            config.get("fgbio_call_overlapping_consensus_bases", {}).get("benchmark_repeats", 1),
        )


use rule samtools_filter_reads from alignment as alignment_samtools_filter_reads with:
    input:
        bam="alignment/bwa_mem/{sample}_{type}.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}_unsorted.bam"),
    log:
        "alignment/samtools_filter_reads/{sample}_{type}_unsorted.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/samtools_filter_reads/{sample}_{type}_unsorted.bam.benchmark.tsv",
            config.get("samtools_filter_reads", {}).get("benchmark_repeats", 1),
        )


use rule samtools_filter_reads from alignment as alignment_samtools_filter_reads_fusions with:
    input:
        bam="alignment/bwa_mem/{sample}_{type}.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}.fusions.bam"),
    params:
        extra=config.get("samtools_filter_reads_fusions", {}).get("extra", ""),
    log:
        "alignment/samtools_filter_reads/{sample}_{type}.fusions.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/samtools_filter_reads/{sample}_{type}.fusions.bam.benchmark.tsv",
            config.get("samtools_filter_reads", {}).get("benchmark_repeats", 1),
        )


use rule samtools_filter_reads from alignment as alignment_samtools_filter_reads_umi with:
    input:
        bam="alignment/bwa_mem/{sample}_{type}.umi.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}.umi.bam"),
    params:
        extra=config.get("samtools_filter_reads_umi", {}).get("extra", ""),
    log:
        "alignment/samtools_filter_reads/{sample}_{type}.umi.bam.log",
    benchmark:
        repeat(
            "alignment/samtools_filter_reads/{sample}_{type}.umi.bam.benchmark.tsv",
            config.get("samtools_filter_reads", {}).get("benchmark_repeats", 1),
        )


use rule samtools_filter_reads from alignment as alignment_samtools_filter_reads_umi_fusions1 with:
    input:
        bam="alignment/bwa_mem/{sample}_{type}.umi.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}.umi.fusions1.bam"),
    params:
        extra=config.get("samtools_filter_reads_umi_fusions1", {}).get("extra", ""),
    log:
        "alignment/samtools_filter_reads/{sample}_{type}.umi.fusions1.bam.log",
    benchmark:
        repeat(
            "alignment/samtools_filter_reads/{sample}_{type}.umi.fusions1.bam.benchmark.tsv",
            config.get("samtools_filter_reads", {}).get("benchmark_repeats", 1),
        )


use rule samtools_filter_reads from alignment as alignment_samtools_filter_reads_umi_fusions2 with:
    input:
        bam="alignment/bwa_mem/{sample}_{type}.umi.bam",
    output:
        bam=temp("alignment/samtools_filter_reads/{sample}_{type}.umi.fusions2.bam"),
    params:
        extra=config.get("samtools_filter_reads_umi_fusions2", {}).get("extra", ""),
    log:
        "alignment/samtools_filter_reads/{sample}_{type}.umi.fusions2.bam.log",
    benchmark:
        repeat(
            "alignment/samtools_filter_reads/{sample}_{type}.umi.fusions2.bam.benchmark.tsv",
            config.get("samtools_filter_reads", {}).get("benchmark_repeats", 1),
        )


use rule picard_mark_duplicates from alignment as alignment_picard_mark_duplicates with:
    input:
        bams=lambda wildcards: "alignment/fgbio_call_overlapping_consensus_bases/{sample}_{type}_{chr}.bam"
        if config.get("run_ffpe_overlapping_consensus", True)
        else "alignment/samtools_extract_reads/{sample}_{type}_{chr}.bam",
    output:
        bam=temp("alignment/picard_mark_duplicates/{sample}_{type}_{chr}.bam"),
        metrics=temp("alignment/picard_mark_duplicates/{sample}_{type}_{chr}.metrics.txt"),
    log:
        "alignment/picard_mark_duplicates/{sample}_{type}_{chr}.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/picard_mark_duplicates/{sample}_{type}_{chr}.bam.benchmark.tsv",
            config.get("picard_mark_duplicates", {}).get("benchmark_repeats", 1),
        )


use rule picard_mark_duplicates from alignment as alignment_picard_mark_duplicates_ffpe_fusions with:
    input:
        bams="alignment/samtools_filter_reads/{sample}_{type}.fusions.bam",
    output:
        bam=temp("alignment/picard_mark_duplicates_ffpe_fusions/{sample}_{type}.bam"),
        metrics=temp("alignment/picard_mark_duplicates_ffpe_fusions/{sample}_{type}.metrics.txt"),
    log:
        "alignment/picard_mark_duplicates_ffpe_fusions/{sample}_{type}.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/picard_mark_duplicates_ffpe_fusions/{sample}_{type}.bam.benchmark.tsv",
            config.get("picard_mark_duplicates", {}).get("benchmark_repeats", 1),
        )


use rule samtools_merge_bam from alignment as alignment_samtools_merge_bam_all_final with:
    input:
        bam=lambda wildcards: expand(
            "alignment/picard_mark_duplicates/{{sample}}_{{type}}_{chr}.bam",
            chr=extract_chr(
                "%s.fai" % (config.get("reference", {}).get("fasta", "")),
                filter_out=config.get("reference", {}).get("skip_chrs", []),
            ),
        )
        + ["alignment/picard_mark_duplicates_ffpe_fusions/{sample}_{type}.bam"],
    output:
        bam="alignment/samtools_merge_bam_all_final/{sample}_{type}.bam",
    log:
        "alignment/samtools_merge_bam_all_final/{sample}_{type}.bam.log",
    wildcard_constraints:
        type="T|N",
    benchmark:
        repeat(
            "alignment/samtools_merge_bam_all_final/{sample}_{type}.bam.benchmark.tsv",
            config.get("samtools_merge_bam", {}).get("benchmark_repeats", 1),
        )


use rule fgbio_call_and_filter_consensus_reads from alignment as alignment_fgbio_call_and_filter_consensus_reads with:
    output:
        bam=temp("alignment/fgbio_call_and_filter_consensus_reads/{sample}_{type}.umi.unmapped_unsorted.bam"),
    log:
        "alignment/fgbio_call_and_filter_consensus_reads/{sample}_{type}.umi.unmapped.bam.log",
    benchmark:
        repeat(
            "alignment/fgbio_call_and_filter_consensus_reads/{sample}_{type}.umi.unmapped.bam.benchmark.tsv",
            config.get("fgbio_call_and_filter_consensus_reads", {}).get("benchmark_repeats", 1),
        )


use rule bwa_mem_realign_consensus_reads from alignment as alignment_bwa_mem_realign_consensus_reads_unmapped with:
    input:
        bam="alignment/fgbio_call_and_filter_consensus_reads/{sample}_{type}.umi.unmapped.bam",
    output:
        bam=temp("alignment/bwa_mem_realign_consensus_reads_unmapped/{sample}_{type}.umi_unsorted.bam"),
    log:
        "alignment/bwa_mem_realign_consensus_reads_unmapped/{sample}_{type}.umi.bam.log",
    benchmark:
        repeat(
            "alignment/bwa_mem_realign_consensus_reads_unmapped/{sample}_{type}.umi.bam.benchmark.tsv",
            config.get("bwa_mem_realign_consensus_reads", {}).get("benchmark_repeats", 1),
        )


use rule picard_mark_duplicates from alignment as alignment_picard_mark_duplicates_umi_fusions1 with:
    input:
        bams="alignment/samtools_filter_reads/{sample}_{type}.umi.fusions1.bam",
    output:
        bam=temp("alignment/picard_mark_duplicates_umi_fusions1/{sample}_{type}.umi.bam"),
        metrics=temp("alignment/picard_mark_duplicates_umi_fusions1/{sample}_{type}.umi.metrics.txt"),
    log:
        "alignment/picard_mark_duplicates_umi_fusions1/{sample}_{type}.umi.bam.log",
    benchmark:
        repeat(
            "alignment/picard_mark_duplicates_umi_fusions1/{sample}_{type}.umi.bam.benchmark.tsv",
            config.get("picard_mark_duplicates", {}).get("benchmark_repeats", 1),
        )


use rule picard_mark_duplicates from alignment as alignment_picard_mark_duplicates_umi_fusions2 with:
    input:
        bams="alignment/samtools_filter_reads/{sample}_{type}.umi.fusions2.bam",
    output:
        bam=temp("alignment/picard_mark_duplicates_umi_fusions2/{sample}_{type}.umi.bam"),
        metrics=temp("alignment/picard_mark_duplicates_umi_fusions2/{sample}_{type}.umi.metrics.txt"),
    log:
        "alignment/picard_mark_duplicates_umi_fusions2/{sample}_{type}.umi.bam.log",
    benchmark:
        repeat(
            "alignment/picard_mark_duplicates_umi_fusions2/{sample}_{type}.umi.bam.benchmark.tsv",
            config.get("picard_mark_duplicates", {}).get("benchmark_repeats", 1),
        )


use rule samtools_merge_bam from alignment as alignment_samtools_merge_bam_umi with:
    input:
        bam=[
            "alignment/bwa_mem_realign_consensus_reads_unmapped/{sample}_{type}.umi_unsorted.bam",
            "alignment/picard_mark_duplicates_umi_fusions1/{sample}_{type}.umi.bam",
            "alignment/picard_mark_duplicates_umi_fusions2/{sample}_{type}.umi.bam",
        ],
    output:
        bam=temp("alignment/samtools_merge_bam_umi/{sample}_{type}.umi.merged_unsorted.bam"),
    log:
        "alignment/samtools_merge_bam_umi/{sample}_{type}.umi.merged_unsorted.bam.log",
    benchmark:
        repeat(
            "alignment/samtools_merge_bam_umi/{sample}_{type}.umi.merged_unsorted.bam.benchmark.tsv",
            config.get("samtools_merge_bam", {}).get("benchmark_repeats", 1),
        )


use rule samtools_sort from alignment as alignment_samtools_sort_umi with:
    input:
        bam="alignment/samtools_merge_bam_umi/{sample}_{type}.umi.merged_unsorted.bam",
    output:
        bam=temp("alignment/bwa_mem_realign_consensus_reads/{sample}_{type}.umi.bam"),
    log:
        "alignment/bwa_mem_realign_consensus_reads/{sample}_{type}.umi.bam.sort.log",
    benchmark:
        repeat(
            "alignment/bwa_mem_realign_consensus_reads/{sample}_{type}.umi.bam.sort.benchmark.tsv",
            config.get("samtools_sort", {}).get("benchmark_repeats", 1),
        )


use rule star from alignment as alignment_star with:
    input:
        fq1="prealignment/merged_arriba/{sample}_{type}_fastq1.fastq.gz",
        fq2="prealignment/merged_arriba/{sample}_{type}_fastq2.fastq.gz",
        idx=config.get("star", {}).get("genome_index", ""),
    params:
        extra=lambda wildcards: "%s %s"
        % (
            config.get("star", {}).get("extra", ""),
            config.get("star", {}).get("read_group", generate_star_read_group(wildcards)),
        ),
        idx="{input.idx}",


use rule samtools_sort from alignment as alignment_samtools_sort_rna with:
    input:
        bam="fusions/star_fusion/{path_file}.out.bam",
    output:
        bam=temp("fusions/star_fusion/{path_file}.out.sorted.bam"),
    log:
        "fusions/star_fusion/{path_file}.bam.sort.log",
    benchmark:
        repeat(
            "fusions/star_fusion/{path_file}.bam.sort.benchmark.tsv",
            config.get("samtools_sort", {}).get("benchmark_repeats", 1),
        )


use rule samtools_extract_reads from alignment as alignment_samtools_extract_reads_rna with:
    input:
        bam="alignment/star/{sample}_{type}.bam",
        bai="alignment/star/{sample}_{type}.bam.bai",
    wildcard_constraints:
        type="R",


use rule samtools_extract_reads from alignment as alignment_samtools_extract_reads with:
    input:
        bam=lambda wildcards: "alignment/samtools_filter_reads/{sample}_{type}.bam"
        if config.get("run_ffpe_overlapping_consensus", True)
        else get_deduplication_bam_input(wildcards),
        bai=lambda wildcards: "alignment/samtools_filter_reads/{sample}_{type}.bam.bai"
        if config.get("run_ffpe_overlapping_consensus", True)
        else get_deduplication_bam_input_bai(wildcards),
    wildcard_constraints:
        type="T|N",


use rule samtools_extract_reads_umi from alignment as alignment_samtools_extract_reads_umi with:
    input:
        bam="alignment/bwa_mem_realign_consensus_reads/{sample}_{type}.umi.bam",
        bai="alignment/bwa_mem_realign_consensus_reads/{sample}_{type}.umi.bam.bai",
    output:
        bam=temp("alignment/samtools_extract_reads_umi/{sample}_{type}_{chr}.umi.bam"),
    wildcard_constraints:
        type="T|N",


use rule samtools_merge_bam from alignment as alignment_samtools_merge_bam_mutect2 with:
    input:
        bams=expand(
            "snv_indels/gatk_mutect2/{{sample}}_{{type}}_{chr}.unfiltered.bam",
            chr=extract_chr(
                "%s.fai" % (config.get("reference", {}).get("fasta", "")),
                filter_out=config.get("reference", {}).get("skip_chrs", []),
            ),
        ),
    output:
        bam=temp("snv_indels/gatk_mutect2_merge/{sample}_{type}_unsorted.bam"),
    params:
        extra=config.get("alignment_samtools_merge_bam_mutect2", {}).get("extra", ""),
    log:
        "snv_indels/gatk_mutect2_merge/{sample}_{type}_unsorted.bam.log",
    benchmark:
        repeat(
            "snv_indels/gatk_mutect2_merge/{sample}_{type}_unsorted.bam.benchmark.tsv",
            config.get("alignment_samtools_merge_bam_mutect2", {}).get("benchmark_repeats", 1),
        )


module snv_indels:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/snv_indels",
            path="workflow/Snakefile",
            tag=config["modules"]["snv_indels"],
        )
    config:
        config


use rule * from snv_indels exclude all as snv_indels_*


use rule bcftools_sort from snv_indels as snv_indels_bcftools_sort with:
    input:
        vcf="{file}.vcf.gz",


use rule vardict from snv_indels as snv_indels_vardict with:
    input:
        bam=get_deduplication_bam_chr_input,
        bai=get_deduplication_bam_chr_input_bai,
        reference=config["reference"]["fasta"],
        regions="snv_indels/bed_split/design_bedfile_{chr}.bed",
    params:
        extra=config.get("vardict", {}).get("extra", "-Q 1"),
        bed_columns=config.get("vardict", {}).get("bed_columns", "-c 1 -S 2 -E 3 -g 4"),
        allele_frequency_threshold=lambda wildcards: get_vardict_min_af(wildcards),
        sample_name="{sample}_{type}",


use rule gatk_mutect2 from snv_indels as snv_indels_gatk_mutect2 with:
    input:
        map=get_deduplication_bam_chr_input,
        bai=get_deduplication_bam_chr_input_bai,
        fasta=config.get("reference", {}).get("fasta", ""),
        bed="snv_indels/bed_split/design_bedfile_{chr}.bed",


use rule gatk_mutect2_gvcf from snv_indels as snv_indels_gatk_mutect2_gvcf with:
    input:
        map=get_deduplication_bam_chr_input,
        bai=get_deduplication_bam_chr_input_bai,
        fasta=config.get("reference", {}).get("fasta", ""),
        bed="snv_indels/bed_split/design_bedfile_{chr}.bed",


module annotation:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/annotation",
            path="workflow/Snakefile",
            tag=config["modules"]["annotation"],
        )
    config:
        config


use rule * from annotation exclude all as annotation_*


use rule bgzip_vcf from annotation as annotation_bgzip_vcf with:
    wildcard_constraints:
        file="^qc/.+",


use rule tabix_vcf from annotation as annotation_tabix_vcf with:
    wildcard_constraints:
        file="^qc/.+",


use rule annotate_cnv from annotation as annotation_annotate_cnv with:
    wildcard_constraints:
        file="^cnv_sv/.+",


use rule vep from annotation as annotation_vep_wo_pick with:
    output:
        vcf=temp("{file}.vep_annotated_wo_pick.vcf"),
    params:
        extra=config.get("vep_wo_pick", {}).get("extra", ""),
        mode=config.get("vep_wo_pick", {}).get("mode", "--offline --cache --refseq "),
    log:
        "{file}.vep_annotated_wo_pick.vcf.log",
    benchmark:
        repeat(
            "{file}.vep_annotated_wo_pick.vcf.benchmark.tsv",
            config.get("vep_wo_pick", {}).get("benchmark_repeats", 1),
        )


use rule bcftools_annotate from annotation as annotation_bcftools_annotate_purecn with:
    input:
        vcf="snv_indels/gatk_mutect2/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.vcf.gz",
        tbi="snv_indels/gatk_mutect2/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.vcf.gz.tbi",
        annotation_db="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",
        annotation_db_tbi="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz.tbi",
    output:
        vcf=temp(
            "snv_indels/gatk_mutect2/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.bcftools_annotated_purecn.vcf"
        ),
    params:
        annotation_db=lambda wildcards, input: input.annotation_db,
        output_type=config.get("bcftools_annotate_purecn", {}).get("output_type", "z"),
        annotation_string=config.get("bcftools_annotate_purecn", {}).get("annotation_string", "-m DB"),
        extra=config.get("bcftools_annotate_purecn", {}).get("extra", ""),
    log:
        "snv_indels/gatk_mutect2/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.bcftools_annotated_purecn.vcf.log",
    benchmark:
        repeat(
            "snv_indels/gatk_mutect2/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.bcftools_annotated_purecn.vcf.benchmark.tsv",
            config.get("bcftools_annotate", {}).get("benchmark_repeats", 1),
        )


module filtering:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/filtering",
            path="workflow/Snakefile",
            tag=config["modules"]["filtering"],
        )
    config:
        config


use rule * from filtering exclude all as filtering_*


use rule filter_vcf from filtering as filtering_filter_vcf with:
    wildcard_constraints:
        file="^cnv_sv/.+|^snv_indels/.+",
        tag="[a-zA-Z0-9-_]+",


use rule bcftools_filter_include_region from filtering as filtering_bcftools_filter_include_region with:
    wildcard_constraints:
        tag="[a-zA-Z0-9-_]+",


module qc:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/qc",
            path="workflow/Snakefile",
            tag=config["modules"]["qc"],
        )
    config:
        config


use rule * from qc exclude all as qc_*


use rule multiqc from qc as qc_multiqc with:
    output:
        html=temp("qc/multiqc/multiqc_{report}.html"),
        data=directory("qc/multiqc/multiqc_{report}_data"),
        data_json="qc/multiqc/multiqc_{report}_data/multiqc_data.json",


use rule mosdepth from qc as qc_mosdepth with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        bed=config.get("reference", {}).get("design_bed_rna", ""),
    wildcard_constraints:
        type="R",


use rule mosdepth_bed from qc as qc_mosdepth_bed with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        bed=config.get("reference", {}).get("design_bed", ""),


use rule samtools_stats from qc as qc_samtools_stats with:
    wildcard_constraints:
        type="T|N",


use rule samtools_stats from qc as qc_samtools_stats_rna with:
    input:
        bam=get_deduplication_bam_input,
    wildcard_constraints:
        type="R",


use rule picard_collect_hs_metrics from qc as qc_picard_collect_hs_metrics with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        bait_intervals=config.get("reference", {}).get("design_intervals", ""),
        reference=config.get("reference", {}).get("fasta", ""),
        target_intervals=config.get("reference", {}).get("design_intervals", ""),
    wildcard_constraints:
        type="T|N",


use rule picard_collect_hs_metrics from qc as qc_picard_collect_hs_metrics_rna with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        bait_intervals=config.get("reference", {}).get("design_intervals_rna", ""),
        reference=config.get("reference", {}).get("fasta", ""),
        target_intervals=config.get("reference", {}).get("design_intervals_rna", ""),
    params:
        extra="%s %s"
        % (
            config.get("picard_collect_hs_metrics", {}).get("extra", ""),
            "VALIDATION_STRINGENCY=LENIENT",
        ),
    wildcard_constraints:
        type="R",


use rule picard_collect_alignment_summary_metrics from qc as qc_picard_collect_alignment_summary_metrics with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        ref=config.get("reference", {}).get("fasta", ""),
    wildcard_constraints:
        type="T|N",


use rule picard_collect_alignment_summary_metrics from qc as qc_picard_collect_alignment_summary_metrics_rna with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        ref=config.get("reference", {}).get("fasta", ""),
    params:
        extra="%s %s"
        % (
            config.get("picard_collect_alignment_summary_metrics", {}).get("extra", ""),
            "VALIDATION_STRINGENCY=LENIENT",
        ),
    wildcard_constraints:
        type="R",


use rule picard_collect_duplication_metrics from qc as qc_picard_collect_duplication_metrics with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,


use rule picard_collect_insert_size_metrics from qc as qc_picard_collect_insert_size_metrics with:
    input:
        bam=get_deduplication_bam_input,


use rule gatk_get_pileup_summaries from qc as qc_gatk_get_pileup_summaries with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        sites=config.get("gatk_get_pileup_summaries", {}).get("sites", ""),
        variants=config.get("gatk_get_pileup_summaries", {}).get("variants", ""),


module biomarker:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/biomarker",
            path="workflow/Snakefile",
            tag=config["modules"]["biomarker"],
        )
    config:
        config


use rule * from biomarker exclude all as biomarker_*


use rule cnvkit2scarhrd from biomarker as biomarker_cnvkit2scarhrd with:
    input:
        seg="cnv_sv/cnvkit_call_hrd/{sample}_{type}.{tc_method}.loh.cns",


use rule msisensor_pro_filter_sites from biomarker as biomarker_msisensor_pro_filter_sites_unfiltered with:
    output:
        PoN=temp("biomarker/msisensor_pro_filter_sites/{sample}_{type}.Msisensor_pro_reference.unfiltered.list_baseline"),
    params:
        msi_sites_bed="",


use rule msisensor_pro from biomarker as biomarker_msisensor_pro_unfiltered with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        PoN="biomarker/msisensor_pro_filter_sites/{sample}_{type}.Msisensor_pro_reference.unfiltered.list_baseline",
    output:
        msi_score=temp("biomarker/msisensor_pro_unfiltered/{sample}_{type}"),
        msi_all=temp("biomarker/msisensor_pro_unfiltered/{sample}_{type}_all"),
        msi_dis=temp("biomarker/msisensor_pro_unfiltered/{sample}_{type}_dis"),
        msi_unstable=temp("biomarker/msisensor_pro_unfiltered/{sample}_{type}_unstable"),
    params:
        extra=config.get("msisensor_pro", {}).get("extra", "-c 50 -b 2"),
        out_prefix="biomarker/msisensor_pro_unfiltered/{sample}_{type}",


use rule tmb from biomarker as biomarker_tmb with:
    input:
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.artifact_annotated.hotspot_annotated.background_annotated.include.exon.filter.snv_hard_filter.vcf.gz",


use rule optitype from biomarker as biomarker_optitype with:
    output:
        coverage_plot=temp("biomarker/optitype/{sample}_{type}/{sample}_{type}_hla_type_coverage_plot.pdf"),
        hla_type=temp("biomarker/optitype/{sample}_{type}/{sample}_{type}_hla_type_result.tsv"),
        out_dir=directory("biomarker/optitype/{sample}_{type}/"),


use rule tmb from biomarker as biomarker_tmb_umi with:
    input:
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.artifact_annotated.hotspot_annotated.background_annotated.include.exon.filter.snv_hard_filter_umi.vcf.gz",
    output:
        tmb=temp("biomarker/tmb/{sample}_{type}.umi.TMB.txt"),
    params:
        af_germline_lower_limit=config.get("tmb_umi", {}).get("af_germline_lower_limit", 0.47),
        af_germline_upper_limit=config.get("tmb_umi", {}).get("af_germline_upper_limit", 0.53),
        af_lower_limit=config.get("tmb_umi", {}).get("af_lower_limit", 0.05),
        af_upper_limit=config.get("tmb_umi", {}).get("af_upper_limit", 0.95),
        artifacts=config.get("tmb_umi", {}).get("artifacts", ""),
        background_panel=config.get("tmb_umi", {}).get("background", ""),
        background_sd_limit=config.get("tmb_umi", {}).get("background_sd_limit", 5),
        db1000g_limit=config.get("tmb_umi", {}).get("db1000g_limit", 0.0001),
        dp_limit=config.get("tmb_umi", {}).get("dp_limit", 200),
        filter_genes=config.get("tmb_umi", {}).get("filter_genes", ""),
        filter_nr_observations=config.get("tmb_umi", {}).get("filter_nr_observations", 1),
        filter_regions=config.get("tmb_umi", {}).get("filter_regions", []),
        gnomad_limit=config.get("tmb_umi", {}).get("gnomad_limit", 0.0001),
        nr_avg_germline_snvs=config.get("tmb_umi", {}).get("nr_avg_germline_snvs", 2.0),
        nssnv_tmb_correction=config.get("tmb_umi", {}).get("nssnv_tmb_correction", 0.84),
        variant_type_list=config.get("tmb", {}).get(
            "variant_type_list", ["missense_variant", "stop_gained", "stop_lost", "synonymous_variant", "stop_retained_variant"]
        ),
        vd_limit=config.get("tmb_umi", {}).get("vd_limit", 20),


module fusions:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/fusions",
            path="workflow/Snakefile",
            tag=config["modules"]["fusions"],
        )
    config:
        config


use rule * from fusions exclude all as fusions_*


rule juli_call:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,


rule fuseq_wes:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        ref_json=config.get("fuseq_wes", {}).get("ref_json", ""),
        gtfSqlite=config.get("fuseq_wes", {}).get("gtfSqlite", ""),
        fusiondb=config.get("fuseq_wes", {}).get("fusiondb", ""),
        paralogdb=config.get("fuseq_wes", {}).get("paralogdb", ""),
        params=config.get("fuseq_wes", {}).get("params", ""),


use rule fuseq_wes from fusions as fusions_fuseq_wes with:
    priority: 50


use rule filter_report_fuseq_wes from fusions as fusions_filter_report_fuseq_wes_umi with:
    output:
        fusions=temp("fusions/filter_fuseq_wes/{sample}_{type}.fuseq_wes.report.umi.csv"),
    params:
        min_support=config.get("filter_fuseq_wes_umi", {}).get("min_support", ""),
        gene_white_list=config.get("filter_fuseq_wes_umi", {}).get("gene_white_list", ""),
        gene_fusion_black_list=config.get("filter_fuseq_wes_umi", {}).get("gene_fusion_black_list", ""),
        transcript_black_list=config.get("filter_fuseq_wes_umi", {}).get("transcript_black_list", ""),
        filter_on_fusiondb=config.get("filter_fuseq_wes_umi", {}).get("filter_on_fusiondb", ""),
        gtf=config.get("filter_fuseq_wes_umi", {}).get("gtf", ""),


use rule star_fusion from fusions as fusions_star_fusion with:
    output:
        bam=temp("fusions/star_fusion/{sample}_{type}/Aligned.out.bam"),
        fusions=temp("fusions/star_fusion/{sample}_{type}/star-fusion.fusion_predictions.tsv"),
        fusions_abridged=temp("fusions/star_fusion/{sample}_{type}/star-fusion.fusion_predictions.abridged.coding_effect.tsv"),
        sj=temp("fusions/star_fusion/{sample}_{type}/SJ.out.tab"),


module cnv_sv:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/cnv_sv",
            path="workflow/Snakefile",
            tag=config["modules"]["cnv_sv"],
        )
    config:
        config


use rule * from cnv_sv exclude all as cnv_sv_*


use rule manta_config_t from cnv_sv as cnv_sv_manta_config_t with:
    input:
        bam_t=get_deduplication_bam_input_manta,
        bai_t=get_deduplication_bam_input_manta_bai,
        ref=config["reference"]["fasta"],
    priority: 40


use rule manta_run_workflow_t from cnv_sv as cnv_sv_manta_run_workflow_t with:
    input:
        bam_t=get_deduplication_bam_input_manta,
        bai_t=get_deduplication_bam_input_manta_bai,
        ref=config["reference"]["fasta"],
        scrpt="cnv_sv/manta_run_workflow_t/{sample}/runWorkflow.py",
    priority: 40


use rule cnvkit_batch from cnv_sv as cnv_sv_cnvkit_batch_hrd with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        reference=config.get("cnvkit_batch_hrd", {}).get("normal_reference_hrd", ""),
    output:
        antitarget_coverage=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.antitargetcoverage.cnn"),
        bins=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.bintest.cns"),
        regions=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.cnr"),
        segments=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.cns"),
        segments_called=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.call.cns"),
        target_coverage=temp("cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.targetcoverage.cnn"),
    log:
        "cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.log",
    benchmark:
        repeat(
            "cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.benchmark.hrd.tsv",
            config.get("cnvkit_batch_hrd", {}).get("benchmark_repeats", 1),
        )


use rule cnvkit_call from cnv_sv as cnv_sv_cnvkit_call_hrd with:
    input:
        segment="cnv_sv/cnvkit_batch_hrd/{sample}/{sample}_{type}.cns",
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",
        tc_file=get_tc_file,
    output:
        segment=temp("cnv_sv/cnvkit_call_hrd/{sample}_{type}.{tc_method}.loh.cns"),
    params:
        purity=get_tc,
        extra=config.get("cnvkit_call_hrd", {}).get("extra", ""),
    log:
        "cnv_sv/cnvkit_call_hrd/{sample}_{type}.{tc_method}.loh.cns.log",
    benchmark:
        repeat(
            "cnv_sv/cnvkit_call_hrd/{sample}_{type}.{tc_method}.loh.cns.benchmark.tsv",
            config.get("cnvkit_call_hrd", {}).get("benchmark_repeats", 1),
        )


use rule cnvkit_batch from cnv_sv as cnv_sv_cnvkit_batch with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        reference=config.get("cnvkit_batch", {}).get("normal_reference", ""),


use rule cnvkit_call from cnv_sv as cnv_sv_cnvkit_call with:
    input:
        segment="cnv_sv/cnvkit_batch/{sample}/{sample}_{type}.cns",
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",
        tc_file=get_tc_file,
    params:
        extra=config.get("cnvkit_call", {}).get("extra", ""),
        purity=get_tc,


use rule jumble_run from cnv_sv as cnv_sv_jumble_run with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",


use rule jumble_cnvkit_call from cnv_sv as cnv_sv_jumble_cnvkit_call with:
    input:
        segment="cnv_sv/jumble_run/{sample}_{type}/{sample}_{type}.bam.cns",
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",
        tc_file=get_tc_file,
    params:
        extra=config.get("jumble_cnvkit_call", {}).get("extra", ""),
        purity=get_tc,


use rule cnvkit_scatter from cnv_sv as cnv_sv_cnvkit_scatter with:
    input:
        segments="cnv_sv/cnvkit_batch/{sample}/{sample}_{type}.cns",
        segment_regions="cnv_sv/cnvkit_batch/{sample}/{sample}_{type}.cnr",
        vcf="snv_indels/bcbio_variation_recall_ensemble/{sample}_{type}.ensembled.vep_annotated.filter.germline.exclude.blacklist.vcf.gz",


use rule gatk_collect_read_counts from cnv_sv as cnv_sv_gatk_collect_read_counts with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        interval=config.get("reference", {}).get("design_intervals_gatk_cnv", ""),


use rule gatk_collect_allelic_counts from cnv_sv as cnv_sv_gatk_collect_allelic_counts with:
    input:
        bam=get_deduplication_bam_input,
        bai=get_deduplication_bam_input_bai,
        interval=config.get("gatk_collect_allelic_counts", {}).get("SNP_interval", ""),
        ref=config["reference"]["fasta"],


use rule gatk_to_vcf from cnv_sv as cnv_sv_gatk_to_vcf with:
    input:
        segment="cnv_sv/gatk_model_segments/{sample}_{type}.clean.modelFinal.seg",
        tc_file=get_tc_file,
    params:
        dup_limit=config.get("gatk_vcf", {}).get("dup_limit", 2.5),
        het_del_limit=config.get("gatk_vcf", {}).get("het_del_limit", 1.5),
        hom_del_limit=config.get("gatk_vcf", {}).get("hom_del_limit", 0.5),
        sample_id="{sample}_{type}",
        tc=get_tc,


use rule purecn from cnv_sv as cnv_sv_purecn with:
    input:
        unpack(cnv_sv.get_purecn_inputs),
        vcf="cnv_sv/purecn_modify_vcf/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.bcftools_annotated_purecn.mbq.vcf.gz",
        tbi="cnv_sv/purecn_modify_vcf/{sample}_{type}.normalized.sorted.vep_annotated.filter.snv_hard_filter_purecn.bcftools_annotated_purecn.mbq.vcf.gz.tbi",
    output:
        csv="cnv_sv/purecn/temp/{sample}_{type}/{sample}_{type}.csv",
        outdir=directory("cnv_sv/purecn/temp/{sample}_{type}/"),


use rule purecn_coverage from cnv_sv as cnv_sv_purecn_coverage with:
    output:
        purecn=expand(
            "cnv_sv/purecn_coverage/{{sample}}_{{type}}{ext}",
            ext=[
                "_coverage.txt.gz",
                "_coverage_loess.txt.gz",
                "_coverage_loess.png",
                "_coverage_loess_qc.txt",
            ],
        ),


use rule purecn_copy_output from cnv_sv as cnv_sv_purecn_copy_output with:
    output:
        files="cnv_sv/purecn/{sample}_{type}{suffix}",


use rule purecn_purity_file from cnv_sv as cnv_sv_purecn_purity_file with:
    output:
        purity="cnv_sv/purecn_purity_file/{sample}_{type}.purity.txt",


module reports:
    snakefile:
        get_module_snakefile(
            config,
            "hydra-genetics/reports",
            path="workflow/Snakefile",
            tag=config["modules"]["reports"],
        )
    config:
        config


use rule * from reports exclude all as reports_*


use rule general_json_report from reports as reports_general_json_report with:
    input:
        files=get_all_report_inputs,
        output_files=config.get("general_report", {}),
    params:
        sample="{sample}_{type}",
        pipeline_version=pipeline_version,
        pipeline_name=pipeline_name,
        tc=reports.get_tc_general_report,
        units=units,
        reference_genome=config.get("reference", {}).get("fasta", ""),


use rule general_html_report from reports as reports_general_html_report_umi with:
    output:
        html="reports/general_html_report/{sample}_{type}.umi.general_report.html",


use rule general_html_report from reports as reports_general_html_report_prio with:
    output:
        html="reports/general_html_report/{sample}_{type}.prio.general_report.html",


use rule cnv_html_report from reports as reports_cnv_html_report with:
    input:
        json=workflow.get_rule("reports_cnv_html_report").input.json,
        html_template=workflow.get_rule("reports_cnv_html_report").input.html_template,
        js_files=[
            workflow.get_rule("reports_cnv_html_report").input.js_files[0],
            workflow.get_rule("reports_cnv_html_report").input.js_files[1],
            workflow.get_rule("reports_cnv_html_report").input.js_files[2],
            workflow.get_rule("reports_cnv_html_report").input.js_files[3],
            workflow.get_rule("reports_cnv_html_report").input.js_files[4],
            workflow.get_rule("reports_cnv_html_report").input.js_files[5],
        ],
        css_files=[
            workflow.get_rule("reports_cnv_html_report").input.css_files[0],
            workflow.get_rule("reports_cnv_html_report").input.css_files[1],
        ],
        tc_file=get_tc_file,
        extra_table_files=[t["path"] for t in config.get("cnv_html_report", {}).get("extra_tables", [])],
    params:
        extra_tables=config.get("cnv_html_report", {}).get("extra_tables", []),
        include_table=config.get("cnv_html_report", {}).get("show_table", True),
        include_cytobands=config.get("cnv_html_report", {}).get("cytobands", True),
        tc=get_tc,
        tc_method=lambda wildcards: wildcards.tc_method,


use rule merge_cnv_json from reports as reports_merge_cnv_json with:
    input:
        json=reports.get_json_for_merge_cnv_json,
        fai=config.get("reference", {}).get("fai", ""),
        annotation_bed=config.get("merge_cnv_json", {}).get("annotations", []),
        germline_vcf=config.get("merge_cnv_json", {}).get("germline_vcf", []),
        filtered_cnv_vcfs=config.get("merge_cnv_json", {}).get("filtered_cnv_vcfs", []),
        cnv_vcfs=config.get("merge_cnv_json", {}).get("unfiltered_cnv_vcfs", []),
        cnv_vcfs_tbi=config.get("merge_cnv_json", {}).get("unfiltered_cnv_vcfs_tbi", []),
        cytobands=config.get("merge_cnv_json", {}).get("cytobands", []),
